\documentclass[12pt]{article}

\usepackage{setspace}
\usepackage{gensymb}

\singlespacing


\usepackage[cmex10]{amsmath}

\usepackage{amsthm}

\usepackage{mathrsfs}
\usepackage{txfonts}
\usepackage{stfloats}
\usepackage{bm}
\usepackage{cite}
\usepackage{cases}
\usepackage{subfig}

\usepackage{longtable}
\usepackage{multirow}

\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{steinmetz}
\usepackage{tikz}
\usepackage{circuitikz}
\usepackage{verbatim}
\usepackage{tfrupee}
\usepackage[breaklinks=true]{hyperref}
\usepackage{graphicx}
\usepackage{tkz-euclide}

\usetikzlibrary{calc,math}
\usepackage{listings}
    \usepackage{color}                                            %%
    \usepackage{array}                                            %%
    \usepackage{longtable}                                        %%
    \usepackage{calc}                                             %%
    \usepackage{multirow}                                         %%
    \usepackage{hhline}                                           %%
    \usepackage{ifthen}                                           %%
    \usepackage{lscape}     
\usepackage{multicol}
\usepackage{chngcntr}

\DeclareMathOperator*{\Res}{Res}

\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}
\renewcommand\thesubsubsection{\thesubsection.\arabic{subsubsection}}

%\renewcommand\thesectiondis{\arabic{section}}
%\renewcommand\thesubsectiondis{\thesectiondis.\arabic{subsection}}
%\renewcommand\thesubsubsectiondis{\thesubsectiondis.\arabic{subsubsection}}


\hyphenation{op-tical net-works semi-conduc-tor}
\def\inputGnumericTable{}                                 %%

\lstset{
%language=C,
frame=single, 
breaklines=true,
columns=fullflexible
}
\begin{document}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}{Example}[section]
\newtheorem{definition}[problem]{Definition}

\newcommand{\BEQA}{\begin{eqnarray}}
\newcommand{\EEQA}{\end{eqnarray}}
\newcommand{\define}{\stackrel{\triangle}{=}}
\bibliographystyle{IEEEtran}
\providecommand{\mbf}{\mathbf}
\providecommand{\pr}[1]{\ensuremath{\Pr\left(#1\right)}}
\providecommand{\qfunc}[1]{\ensuremath{Q\left(#1\right)}}
\providecommand{\sbrak}[1]{\ensuremath{{}\left[#1\right]}}
\providecommand{\lsbrak}[1]{\ensuremath{{}\left[#1\right.}}
\providecommand{\rsbrak}[1]{\ensuremath{{}\left.#1\right]}}
\providecommand{\brak}[1]{\ensuremath{\left(#1\right)}}
\providecommand{\lbrak}[1]{\ensuremath{\left(#1\right.}}
\providecommand{\rbrak}[1]{\ensuremath{\left.#1\right)}}
\providecommand{\cbrak}[1]{\ensuremath{\left\{#1\right\}}}
\providecommand{\lcbrak}[1]{\ensuremath{\left\{#1\right.}}
\providecommand{\rcbrak}[1]{\ensuremath{\left.#1\right\}}}
\theoremstyle{remark}
\newtheorem{rem}{Remark}
\newcommand{\sgn}{\mathop{\mathrm{sgn}}}
\providecommand{\abs}[1]{\left\vert#1\right\vert}
\providecommand{\res}[1]{\Res\displaylimits_{#1}} 
\providecommand{\norm}[1]{\left\lVert#1\right\rVert}
%\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\mtx}[1]{\mathbf{#1}}
\providecommand{\mean}[1]{E\left[ #1 \right]}
\providecommand{\fourier}{\overset{\mathcal{F}}{ \rightleftharpoons}}
%\providecommand{\hilbert}{\overset{\mathcal{H}}{ \rightleftharpoons}}
\providecommand{\system}{\overset{\mathcal{H}}{ \longleftrightarrow}}
	%\newcommand{\solution}[2]{\textbf{Solution:}{#1}}
\newcommand{\solution}{\noindent \textbf{Solution: }}
\newcommand{\cosec}{\,\text{cosec}\,}
\providecommand{\dec}[2]{\ensuremath{\overset{#1}{\underset{#2}{\gtrless}}}}
\newcommand{\myvec}[1]{\ensuremath{\begin{pmatrix}#1\end{pmatrix}}}
\newcommand{\mydet}[1]{\ensuremath{\begin{vmatrix}#1\end{vmatrix}}}
\numberwithin{equation}{subsection}
\makeatletter
\@addtoreset{figure}{problem}
\makeatother
\let\StandardTheFigure\thefigure
\let\vec\mathbf
\renewcommand{\thefigure}{\theproblem}
\def\putbox#1#2#3{\makebox[0in][l]{\makebox[#1][l]{}\raisebox{\baselineskip}[0in][0in]{\raisebox{#2}[0in][0in]{#3}}}}
     \def\rightbox#1{\makebox[0in][r]{#1}}
     \def\centbox#1{\makebox[0in]{#1}}
     \def\topbox#1{\raisebox{-\baselineskip}[0in][0in]{#1}}
     \def\midbox#1{\raisebox{-0.5\baselineskip}[0in][0in]{#1}}
\vspace{3cm}
\title{AI5000 - Foundation of Machine Learning Assignment-1 Question-1}
\author{Kranthi Kumar P (AI20RESCH14002) and Zeeshan MD (SM20MTECH12011)}
\date{October 2020}
\maketitle
\newpage
\bigskip
\renewcommand{\thefigure}{\theenumi}
\renewcommand{\thetable}{\theenumi}
%Download the python code for from 
%\begin{lstlisting}
%https://github.com/kranthiakssy/AI20RESCH14002_PhD_IITH/tree/master/%EE5609_Matrix_Theory/Assignment-9
%\end{lstlisting}

%Download the latex-file codes from 
%
%\begin{lstlisting}
%https://github.com/kranthiakssy/AI20RESCH14002_PhD_IITH/tree/master/%EE5609_Matrix_Theory/Assignment-9
%\end{lstlisting}
\section*{Question-1[Theory]: Linear Regression}
%\subsection*{Problem:}
%SVD:\\
%Find the foot of the perpendicular to the plane
%\begin{align}
%2x+3y-2z+4 = 0 \label{eq:equation}
%\end{align}
%from the point $\myvec{3\\-2\\0}$ using SVD.
\subsection*{Solution:}
This is the case of multivariate regression.\\
$(y_0,y_1,\ldots,y_n)$ are dependent variables and $(x_0,x_1,\ldots,x_n)$ are independent variables.\\
All the dependent variables depends on some set of independent variables. So that every independent noise is added to each dimension of the independent variable set.\\
The model can be written as 
\begin{align}
y^{\prime}(x_n,w) = w_o + \sum_{i=1}^{N} w_i (x_{ni}+\epsilon_{ni})\\
= w_o + \sum_{i=1}^{N} w_ix_{ni} + \sum_{i=1}^{N} w_i\epsilon_{ni}\\
= y(x_n,w) + \sum_{i=1}^{N} w_i\epsilon_{ni}\label{eq:yprime}
\end{align}
The Error function is
\begin{align}
E_D^{\prime}(w) = \frac{1}{2} \sum_{n=1}^{N}\left\lbrace y^{\prime}(x_n,w)-t_n\right\rbrace^2\\
= \frac{1}{2} \sum_{n=1}^{N} \left\lbrace y(x_n,w) + \sum_{i=1}^{N} w_i\epsilon_{ni}- t_n \right\rbrace^2 \label{eq:edprime}
\end{align}
expanding the eq \eqref{eq:edprime}
\begin{align}
= \frac{1}{2} \sum_{n=1}^{N}\left\lbrace \left(y(x_n,w) - t_n \right)^2 + 2 \left(y(x_n,w) - t_n \right) \left(\sum_{i=1}^{N} w_i\epsilon_{ni}\right) + \left(\sum_{i=1}^{N} w_i\epsilon_{ni}\right)^2 \right\rbrace \label{eq:ed}
\end{align}
Taking the expectation of eq \eqref{eq:ed}
\begin{align}
\mathbb{E}[E_D^{\prime}] = \frac{1}{2} \sum_{n=1}^{N}\left\lbrace \left(y(x_n,w) - t_n \right)^2 + 2 \left(y(x_n,w) - t_n \right) \left(\sum_{i=1}^{N} w_i \mathbb{E}\left[\epsilon_{ni}\right]\right) + \mathbb{E}\left[\left(\sum_{i=1}^{N} w_i\epsilon_{ni}\right)^2\right] \right\rbrace\label{eq:edexp}
\end{align}
given 
\begin{align}
\mathbb{E}[\epsilon_{ni}] = 0\label{eq:mean}
\end{align}
Now expanding the $\mathbb{E}\left[\left(\sum_{i=1}^{N} w_i\epsilon_{ni}\right)^2\right]$ we get
\begin{align}
\mathbb{E}\left[\left(\sum_{i=1}^{N} w_i\epsilon_{ni}\right)^2\right] = \mathbb{E}\left[\sum_{i=1}^{N} \sum_{j=1}^{N} w_i w_j \epsilon_{ni} \epsilon_{nj} \right]\\
= \sum_{i=1}^{N} \sum_{j=1}^{N} w_i w_j \mathbb{E}\left[\epsilon_{ni} \epsilon_{nj} \right]\\
= \sum_{i=1}^{N} \sum_{j=1}^{N} w_i w_j \delta_{ij} \sigma^2\\
= \sum_{i=1}^{N} w_i^2 \label{eq:reg}
\end{align}
substituting eq \eqref{eq:reg} and \eqref{eq:mean} in eq \eqref{eq:edexp}
\begin{align}
\mathbb{E}\left[E_D^{\prime}(w)\right] = \frac{1}{2} \sum_{n=1}^{N}\left[y(x_n,w) - t_n \right]^2 + \sum_{i=1}^{N} w_i^2\\
= E_D(w) + \frac{N}{2} \sum_{i=1}^{N} w_i^2
\end{align}
$\therefore$ we get a $L_2$ regularization term without the bias parameter $w_o$.
\end{document}